{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook helps to execute the central learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging, json, torch, time\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from configuration import Configuration\n",
    "from os.path import join as path\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "simplefilter(action=\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "from model import Model\n",
    "\n",
    "# Configuration\n",
    "c = Configuration()\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Choose nn device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controll Panel\n",
    "\n",
    "Here you can select a scenario to reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_CASE_FEATURES = {\n",
    "    \"FC-baseline\": c.flowstats + c.pstats,\n",
    "    \"FC-flowstats\": c.flowstats,\n",
    "    \"FC-pstats\": c.pstats,\n",
    "    \"FC-subpstats\": c._pstats_subdirs,\n",
    "    \"FC-subflowstats\": c.pflowstats,\n",
    "    \"FC-All\": c.flowstats + c.pstats + c._pstats_subdirs + c.pflowstats\n",
    "}\n",
    "\n",
    "\n",
    "SCENARIO_NAME = \"FC-All\" # Change this\n",
    "FEATURES = CL_CASE_FEATURES[SCENARIO_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparameters and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path(c.path_results, \"scenarios\", \"CL\", SCENARIO_NAME), exist_ok=True)\n",
    "\n",
    "# Hiperparameters\n",
    "random_state = c.random_state\n",
    "epochs = c.epochs\n",
    "batch_size = c.batch_size_cl\n",
    "lr = c._lr_cl\n",
    "early_stopping_patience = 5\n",
    "\n",
    "# Features\n",
    "features = FEATURES\n",
    "columns = features + c.appl\n",
    "\n",
    "in_features = len(features)\n",
    "out_features = len(c.classes)\n",
    "\n",
    "# Model\n",
    "model = Model(in_features, out_features).to(device)\n",
    "learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total Learnable Parameters: {learnable_params}\")\n",
    "\n",
    "# Hiperparameters 2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Metric vars\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "train_time = []\n",
    "\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "best_val_loss_epoch = -1\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "val_metric_acc = []\n",
    "val_metric_recall = []\n",
    "val_metric_precision = []\n",
    "val_metric_f1 = []\n",
    "val_metric_cm = []\n",
    "\n",
    "test_metric_f1 = None\n",
    "test_metric_acc = None\n",
    "test_metric_precision = None\n",
    "test_metric_recall = None\n",
    "test_metric_cm = None\n",
    "test_metric_cr = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    torch.save(model.state_dict(), path(c.path_results, \"scenarios\", \"CL\", SCENARIO_NAME, \"best_model.torch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model = Model(in_features, out_features).to(device)\n",
    "    model.load_state_dict(torch.load(path(c.path_results, \"scenarios\", \"CL\", SCENARIO_NAME, \"best_model.torch\")))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dead neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dead_neurons_percent(loader):\n",
    "    dead_neurons = {}\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            dead_neurons[module] = (output == 0).all(dim=0).sum().item()\n",
    "\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.ReLU):  # Track ReLU activations\n",
    "            hooks.append(module.register_forward_hook(hook_fn))\n",
    "\n",
    "    # Pass data through model to record activations\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            model(x)\n",
    "            break  # Only need one batch to estimate dead neurons\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return sum(dead_neurons.values())/learnable_params*100  # Total and per-layer dead neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We load the training set in lesser chunks to fit into memory. Else the operating system would swap between HDD/SSD and RAM reducing speed greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FQDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths\n",
    "pth = path(c.path_dataset, \"4-dataset\", \"CL\")\n",
    "    \n",
    "train_cnt = 0\n",
    "\n",
    "for filename in os.listdir(pth):\n",
    "    if filename.startswith(\"train-\"):\n",
    "        train_cnt += 1\n",
    "\n",
    "train_paths = [ path(c.path_dataset, \"4-dataset\", \"CL\", f\"train-{i}.parquet\") for i in range(1, train_cnt+1)]\n",
    "test_path = path(c.path_dataset, \"4-dataset\", \"CL\", \"test.parquet\")\n",
    "validation_path = path(c.path_dataset, \"4-dataset\", \"CL\", \"validation.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df= pd.read_parquet(path, columns=columns)\n",
    "\n",
    "    X, y = df.drop(columns=c.appl), df[c.app]\n",
    "\n",
    "    ds = FQDataset(X, y)\n",
    "\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test/Validation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global train_paths\n",
    "    model.train()\n",
    "            \n",
    "    train_loss_local = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    train_cnt = 0\n",
    "    train_len = 0\n",
    "    for train_path in train_paths:\n",
    "        train_loader = load_data(train_path)\n",
    "        train_cnt += 1\n",
    "        train_len += len(train_loader)\n",
    "\n",
    "        # Training loop\n",
    "        t_start = time.time()\n",
    "        with tqdm(train_loader, f\"Epoch {epoch+1}/{epochs} - Train {train_cnt}/{len(train_paths)} - Training\", unit=\"batch\") as train_bar:\n",
    "            for x, y in train_bar:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(x)\n",
    "                loss = loss_fn(outputs, y)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss_local += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_train += (predicted == y).sum().item()\n",
    "                total_train += y.size(0)\n",
    "                \n",
    "                # Update progress bar with loss\n",
    "                train_bar.set_postfix(train_accuracy=correct_train/total_train, train_loss=train_loss_local/train_len)\n",
    "        \n",
    "        del train_loader\n",
    "            \n",
    "    train_time.append(time.time() - t_start)\n",
    "    train_loss.append(train_loss_local/train_len)\n",
    "    train_acc.append(correct_train/total_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch):\n",
    "    global best_val_loss, best_val_loss_epoch, validation_path\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    val_loss_local = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    val_loader = load_data(validation_path)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_loader, f\"Epoch {epoch+1}/{epochs} - Validating\", unit=\"batch\") as validation_bar:\n",
    "            for x, y in validation_bar:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(x)\n",
    "                loss = loss_fn(outputs, y)\n",
    "                \n",
    "                \n",
    "                val_loss_local += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == y).sum().item()\n",
    "                total_val += y.size(0)\n",
    "                \n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "                validation_bar.set_postfix(validation_accuracy=correct_val/total_val, \n",
    "                                            validation_loss=val_loss_local/len(validation_bar))\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds, normalize='true')\n",
    "    \n",
    "    val_acc.append(correct_val/total_val)\n",
    "    val_loss.append(val_loss_local/len(validation_bar))\n",
    "    \n",
    "    val_metric_acc.append(acc)\n",
    "    val_metric_precision.append(precision)\n",
    "    val_metric_recall.append(recall)\n",
    "    val_metric_f1.append(f1)\n",
    "    val_metric_cm.append(cm)\n",
    "    \n",
    "    if val_loss_local < best_val_loss:\n",
    "        best_val_loss = val_loss_local\n",
    "        best_val_loss_epoch = epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    global test_metric_f1, test_metric_acc, test_metric_precision, test_metric_recall, test_metric_cm, test_metric_cr, test_path\n",
    "    model.eval()\n",
    "        \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    test_loader = load_data(test_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, f\"Testing\", unit=\"batch\") as test_bar:\n",
    "            for x, y in test_bar:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(x)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                \n",
    "                test_bar.set_postfix()\n",
    "        \n",
    "        \n",
    "    # Calculate metrics\n",
    "\n",
    "    int_to_cls = c.load_mapping(\"int_to_cls\")\n",
    "\n",
    "    all_labels = [int_to_cls[x] for x in all_labels]\n",
    "    all_preds = [int_to_cls[x] for x in all_preds]\n",
    "\n",
    "    test_metric_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    test_metric_acc = accuracy_score(all_labels, all_preds)\n",
    "    test_metric_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    test_metric_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    test_metric_cm = confusion_matrix(all_labels, all_preds, labels=c.classes, normalize='true')\n",
    "    test_metric_cr = classification_report(all_labels, all_preds, labels=c.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "    validate(epoch)\n",
    "    \n",
    "    # Early stopping\n",
    "    if epoch == best_val_loss_epoch:\n",
    "        save_model()\n",
    "    elif best_val_loss_epoch - epoch >= early_stopping_patience:\n",
    "        logger.info(\"Early stopping... %s loading best model from epoch %s\", best_val_loss_epoch)\n",
    "        break\n",
    "\n",
    "best_val_loss_epoch += 1\n",
    "\n",
    "# Test with the best model\n",
    "model = load_model()\n",
    "test()\n",
    "\n",
    "# Look for dead neurons\n",
    "val_loader = load_data(validation_path)\n",
    "print(\"Dead neurons %: \", count_dead_neurons_percent(val_loader))\n",
    "dead_neurons_percent = count_dead_neurons_percent(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save raw metrics.json, configuration.json and classiciation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path(c.path_results, \"scenarios\", \"CL\", SCENARIO_NAME, \"metrics.json\"), 'w') as f:\n",
    "    metrics = {\n",
    "        \"epochs\"            : len(train_acc),\n",
    "        \n",
    "        \"train_time_avg_seconds\"    : sum(train_time)/len(train_time),\n",
    "        \"learnable_parameters\"      : learnable_params,\n",
    "        \"best_val_loss\"             : best_val_loss,\n",
    "        \"best_val_loss_epoch\"       : best_val_loss_epoch,\n",
    "        \"dead_neurons_percent\"      : dead_neurons_percent,\n",
    "        \n",
    "        \"test_acc\"          : test_metric_acc,\n",
    "        \"test_precision\"    : test_metric_precision,\n",
    "        \"test_recall\"       : test_metric_recall,\n",
    "        \"test_f1\"           : test_metric_f1,\n",
    "        \n",
    "        \"train_acc\"         : train_acc,\n",
    "        \"train_loss\"        : train_loss,\n",
    "        \"train_time\"        : train_time,\n",
    "        \n",
    "        \"val_acc\"           : val_metric_acc,\n",
    "        \"val_precision\"     : val_metric_precision,\n",
    "        \"val_recall\"        : val_metric_recall,\n",
    "        \"val_f1\"            : val_metric_f1,\n",
    "    }\n",
    "    json.dump(metrics, f, indent=4)\n",
    "    \n",
    "with open(path(c.path_results, \"scenarios\", \"CL\", SCENARIO_NAME, \"configuration.json\"), 'w') as f:\n",
    "    config = {\n",
    "        \"epochs\"                    : epochs,\n",
    "        \"batch_size\"                : batch_size,\n",
    "        \"learning_rate\"             : lr,\n",
    "        \"early_stopping_patience\"   : early_stopping_patience,\n",
    "        \"learnable_parameters\"      : learnable_params,\n",
    "        \"scenario_name\"             : SCENARIO_NAME,\n",
    "        \"device\"                    : device,\n",
    "        \"random_state\"              : random_state,\n",
    "    }\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "with open(path(c.path_results, \"scenarios\", \"CL\", SCENARIO_NAME, \"classification_report.txt\"), 'w') as f:\n",
    "    f.write(test_metric_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Set the font and color palette\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'CMU Serif'\n",
    "plt.rcParams['font.size'] = '14'\n",
    "color_palette = sns.color_palette(\"bright\")\n",
    "    \n",
    "metrics = {\n",
    "    \"Training Accuracy\"    : train_acc,\n",
    "    \"Training Loss\"        : train_loss,\n",
    "    \"Validation Accuracy\"  : val_acc,\n",
    "    \"Validation Loss\"      : val_loss\n",
    "}\n",
    "\n",
    "x_values = np.arange(1, len(train_acc)+1)\n",
    "\n",
    "# Initialize list for lines and labels\n",
    "lines = []\n",
    "labels = []\n",
    "\n",
    "linestyles = ['--', '--', ':', ':']\n",
    "\n",
    "for i, (metric, values) in enumerate(metrics.items()):\n",
    "    line, = plt.plot(x_values\n",
    "                    , values\n",
    "                    , marker='x' if \"Accuracy\" in metric else \".\" \n",
    "                    , color=color_palette[i]\n",
    "                    , linestyle=linestyles[i % len(linestyles)] # adding linestyles\n",
    "                    , alpha=0.8  # reducing alpha\n",
    "                    , zorder=len(metrics)-i  # setting zorder\n",
    "                    , linewidth=1.0\n",
    "                    )\n",
    "    lines.append(line)\n",
    "    labels.append(f'{metric}')\n",
    "\n",
    "# Add best epoch\n",
    "plt.axvline(x = best_val_loss_epoch, color='green', linestyle='-', linewidth=0.75, alpha=0.6)\n",
    "\n",
    "# Add title and labels\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(lines, labels,\n",
    "        bbox_to_anchor=(0., 1.02, 1., .102),\n",
    "        loc='lower left',\n",
    "        ncol=2,\n",
    "        mode=\"expand\",\n",
    "        borderaxespad=0.,\n",
    "        fancybox=True,\n",
    "        shadow=True)\n",
    "\n",
    "plt.grid(linestyle='--', linewidth=0.5)\n",
    "plt.xticks(x_values)\n",
    "plt.tight_layout()\n",
    "plt.savefig(path(c.path_results, \"scenarios\", \"CL\", SCENARIO_NAME, \"fig_acc_loss_all.png\"), dpi=600, bbox_inches='tight')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "        \n",
    "# Set the font and color palette\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'CMU Serif'\n",
    "plt.rcParams['font.size'] = '14'\n",
    "color_palette = sns.color_palette(\"bright\")\n",
    "    \n",
    "metrics = {\n",
    "    \"Accuracy\"          : val_metric_acc,\n",
    "    \"Recall\"            : val_metric_recall,\n",
    "    \"Precision\"         : val_metric_precision,\n",
    "    \"F$_1$-score\"       : val_metric_f1\n",
    "}\n",
    "\n",
    "x_values = np.arange(1, len(train_acc)+1)\n",
    "\n",
    "# Initialize list for lines and labels\n",
    "lines = []\n",
    "labels = []\n",
    "\n",
    "linestyles = ['--', '--', ':', ':']\n",
    "\n",
    "for i, (metric, values) in enumerate(metrics.items()):\n",
    "    line, = plt.plot(x_values\n",
    "                    , values\n",
    "                    , marker='x'\n",
    "                    , color=color_palette[i]\n",
    "                    , linestyle=linestyles[i % len(linestyles)] # adding linestyles\n",
    "                    , alpha=0.8  # reducing alpha\n",
    "                    , zorder=len(metrics)-i  # setting zorder\n",
    "                    , linewidth=1.0\n",
    "                    )\n",
    "    lines.append(line)\n",
    "    labels.append(f'{metric}')\n",
    "\n",
    "# Add best epoch\n",
    "plt.axvline(x= best_val_loss_epoch, color='green', linestyle='-', linewidth=0.75, alpha=0.6)\n",
    "\n",
    "# Add title and labels\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(lines, labels,\n",
    "        bbox_to_anchor=(0., 1.02, 1., .102),\n",
    "        loc='lower left',\n",
    "        ncol=2,\n",
    "        mode=\"expand\",\n",
    "        borderaxespad=0.,\n",
    "        fancybox=True,\n",
    "        shadow=True)\n",
    "\n",
    "plt.grid(linestyle='--', linewidth=0.5)\n",
    "plt.xticks(x_values)\n",
    "plt.tight_layout()\n",
    "plt.savefig(path(c.path_results, \"scenarios\", \"CL\", SCENARIO_NAME, \"fig_val_metrics.png\"), dpi=600, bbox_inches='tight')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSIZE = 14\n",
    "\n",
    "# Set the font family and size\n",
    "plt.rcParams['font.family'] = 'CMU Serif'\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "cmap = sns.color_palette(\"Blues\", as_cmap=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "heatmap = sns.heatmap(test_metric_cm\n",
    "                    , annot=True\n",
    "                    , fmt=\".4f\"\n",
    "                    , linewidths=.5\n",
    "                    , ax=ax\n",
    "                    , cmap=cmap\n",
    "                    , cbar=True\n",
    "                    , rasterized=False\n",
    "                    )\n",
    "\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=FSIZE)\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set(visible=True, lw=.8, edgecolor=\"black\")\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels(c.classes, rotation=45, ha='center', fontsize=FSIZE)  # Rotate the x-axis tick labels by 45 degrees\n",
    "ax.set_yticklabels(c.classes, rotation=45, ha='right', fontsize=FSIZE)  # Rotate the y-axis tick labels by 45 degrees\n",
    "ax.set_xlabel('pred', fontsize=FSIZE)\n",
    "ax.set_ylabel('true', fontsize=FSIZE)\n",
    "\n",
    "# Move the x-axis tick labels to the top\n",
    "ax.xaxis.set_tick_params(labeltop=True, labelbottom=False)\n",
    "\n",
    "# Move the x-axis ticks to the top\n",
    "ax.tick_params(axis='x', which='both', top=True, bottom=False)\n",
    "# ax.tick_params(axis='x', which='both', top=False, bottom=False)\n",
    "# ax.yaxis.set_tick_params(length=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(path(c.path_results, \"scenarios\", \"CL\", SCENARIO_NAME, \"fig_confusion_matrix.png\"), dpi=600, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dipterv_python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
