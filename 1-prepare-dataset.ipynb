{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook creates the federated dataset step-by-step for central and federated learning. The steps are:\n",
    "- Rename CESNET-QUIC22 files\n",
    "- Filter them for target applications and organizations\n",
    "- Columnize the PPI and histogram fields\n",
    "- Calculate and add additional features such as SUBPSTATS and SUBFLOWSTATS\n",
    "- Preprocess then create the central dataset and client datasets\n",
    "- Create the chunks for federated learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os, json, logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipaddress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../\")))\n",
    "from configuration import Configuration\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import join as path\n",
    "from warnings import simplefilter\n",
    "from functools import partial, lru_cache\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "c = Configuration()\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename dataset files\n",
    "\n",
    "This code will rename the daily files to be able to process them more efficently.\n",
    "\n",
    "New flow file for each day: **flows.csv.gz**\n",
    "\n",
    "New stats file for each day: **stats.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files():\n",
    "    for week in c.weeks:\n",
    "        for day in c.days:\n",
    "            for filename in os.listdir(path(c.path_dataset, \"0-cesnet-quic22\", week, day)):\n",
    "                if \".csv\" in filename:\n",
    "                    os.rename(path(c.path_dataset, \"0-cesnet-quic22\", week, day, filename), path(c.path_dataset, \"0-cesnet-quic22\", week, day, \"flows.csv.gz\"))\n",
    "                elif \".json\" in filename:\n",
    "                    os.rename(path(c.path_dataset, \"0-cesnet-quic22\", week, day, filename), path(c.path_dataset, \"0-cesnet-quic22\", week, day, \"stats.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Filter\n",
    "Filter the dataset for target applications and organizations. If you don't have access to the prefixes-orgs.csv, then this code will randomly determine the organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_network(ip, networks):\n",
    "    ip_obj = ipaddress.ip_address(ip)\n",
    "    best_match = None\n",
    "    \n",
    "    for network in networks:\n",
    "        if ip_obj in network:\n",
    "            if best_match is None or network.prefixlen > best_match.prefixlen:\n",
    "                best_match = network\n",
    "\n",
    "    return networks[best_match] if best_match else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ds(week, day, index):\n",
    "    os.makedirs(path(c.path_dataset, \"1-filtered\"), exist_ok=True)\n",
    "    \n",
    "    PATH_DS = path(c.path_dataset, \"0-cesnet-quic22\", week, day, \"flows.csv.gz\")\n",
    "    PATH_DS_OUTPUT = path(c.path_dataset, \"1-filtered\", f\"day-{index}.parquet\")\n",
    "    \n",
    "    ## Load data\n",
    "    logger.info(f\"Loading data for week {week} and day {day}\")\n",
    "    \n",
    "    df = pd.read_csv( PATH_DS, compression=\"gzip\", dtype={\"APP\":\"category\", \"CATEGORY\":\"category\"})\n",
    "    \n",
    "    df_len = len(df)\n",
    "    \n",
    "    ## Filter app\n",
    "    logger.info(f\" Filtering target applications\")\n",
    "    df = df[df[\"APP\"].isin(c.classes)]\n",
    "    \n",
    "    ## Remove Unknown Orgs\n",
    "    try:\n",
    "        logger.info(f\" Determining organizations\")\n",
    "        prefixes = pd.read_csv(\"prefixes-orgs.csv\")\n",
    "        prefixes[\"Prefix\"] = prefixes[\"Prefix\"].map(lambda x: ipaddress.ip_network(x))\n",
    "        prefixes_dict = prefixes.set_index(\"Prefix\")[\"Organization ID\"].to_dict()\n",
    "\n",
    "        find_best_network_fn = lru_cache(maxsize=100000)(partial(find_best_network, networks=prefixes_dict))\n",
    "        df[c.org] = df[\"SRC_IP\"].map(find_best_network_fn)\n",
    "    except:\n",
    "        df[c.org] = np.random.randint(1, 15+1, size=len(df))\n",
    "        df[c.org] = df[c.org].apply(lambda x: -1 if x == 15 else x)\n",
    "    \n",
    "    logger.info(f\" Removing unknown organizations\")\n",
    "    df = df[df[c.org] != -1]\n",
    "    \n",
    "    ## Save data\n",
    "    logger.info(f\" Finished... saving...\")\n",
    "    df.to_parquet(PATH_DS_OUTPUT)\n",
    "    \n",
    "    df_filtered_len = len(df)\n",
    "    \n",
    "    logger.info(f\"Filtering: {df_len - df_filtered_len} rows removed. Reduced dataset by {(df_len - df_filtered_len) / df_len * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "for week in c.weeks:\n",
    "    for day in c.days:\n",
    "        filter_ds(week, day, index)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Eval\n",
    "Eval the list fields into multiple columns and keep target applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ds(index):\n",
    "    os.makedirs(path(c.path_dataset, \"2-eval\"), exist_ok=True)\n",
    "    \n",
    "    PATH_DS_OUTPUT = path(c.path_dataset, \"2-eval\", f\"day-{index}.parquet\")\n",
    "    \n",
    "    ## Load data\n",
    "    \n",
    "    df = c.load_data(\"1-filtered\", index)\n",
    "\n",
    "    # Takes a lot of time\n",
    "    logger.debug(\"eval_ds(): apply eval PPI\")\n",
    "    df[\"PPI\"] = df.apply(lambda x: eval(str(x[\"PPI\"])), axis=1)\n",
    "\n",
    "    # For each packet value we create a new column. If it has less than 30 packets, we are padding it with 0 values until it has.\n",
    "\n",
    "    logger.debug(\"eval_ds(): create 3x30 columns\")\n",
    "    for i in range(0,30):\n",
    "        df[f\"PIAT_{i+1}\"] = df['PPI'].apply(lambda x: x[0][i] if len(x[0]) > i else 0)\n",
    "    for i in range(0,30):\n",
    "        df[f\"DIR_{i+1}\"]  = df['PPI'].apply(lambda x: x[1][i] if len(x[1]) > i else 0)\n",
    "    for i in range(0,30):\n",
    "        df[f\"PS_{i+1}\"]   = df['PPI'].apply(lambda x: x[2][i] if len(x[2]) > i else 0)\n",
    "    \n",
    "    df = df.drop([\"PPI\"], axis=1)\n",
    "\n",
    "    # For each histogram value we create a new column. Each histogram has 8 values.\n",
    "    logger.debug(\"eval_ds(): histogram to columns\")\n",
    "    for col in [\"PHIST_SRC_SIZES\",\"PHIST_DST_SIZES\",\"PHIST_SRC_IPT\",\"PHIST_DST_IPT\"]:\n",
    "        df[col] = df.apply(lambda x: eval(str(x[col])), axis=1)\n",
    "        for i in range(0,8):\n",
    "            df[f\"{col}_{i+1}\"] = df[col].apply(lambda x: x[i])\n",
    "        \n",
    "        df = df.drop([col], axis=1)\n",
    "    \n",
    "    df.to_parquet(PATH_DS_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 15:44:17,302 [DEBUG] eval_ds(): apply eval PPI\n",
      "2025-06-04 15:45:10,495 [DEBUG] eval_ds(): create 3x30 columns\n",
      "2025-06-04 15:45:31,974 [DEBUG] eval_ds(): histogram to columns\n",
      "2025-06-04 15:46:24,040 [DEBUG] eval_ds(): apply eval PPI\n",
      "2025-06-04 15:47:23,797 [DEBUG] eval_ds(): create 3x30 columns\n",
      "2025-06-04 15:47:47,313 [DEBUG] eval_ds(): histogram to columns\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ejozric/Dipterv/FL-QUIC-TC/datasets/1-filtered/day-3.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43meval_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36meval_ds\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m      4\u001b[0m PATH_DS_OUTPUT \u001b[38;5;241m=\u001b[39m path(c\u001b[38;5;241m.\u001b[39mpath_dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2-eval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m## Load data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1-filtered\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Takes a lot of time\u001b[39;00m\n\u001b[1;32m     11\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_ds(): apply eval PPI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Dipterv/FL-QUIC-TC/configuration.py:537\u001b[0m, in \u001b[0;36mConfiguration.load_data\u001b[0;34m(self, dataset, index, columns)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset, index, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mday-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mindex\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dipterv/dipterv_python_venv/lib/python3.12/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dipterv/dipterv_python_venv/lib/python3.12/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/Dipterv/dipterv_python_venv/lib/python3.12/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Dipterv/dipterv_python_venv/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ejozric/Dipterv/FL-QUIC-TC/datasets/1-filtered/day-3.parquet'"
     ]
    }
   ],
   "source": [
    "for index in range(1, 28+1):\n",
    "    eval_ds(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Additional features\n",
    "Calculate additional flow statistics and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_ds(index):\n",
    "    logger.debug(\"features_ds(): index: {}\".format(index))\n",
    "    \n",
    "    PATH_DS_OUTPUT = path(c.path_dataset, \"3-features\", f\"day-{index}.parquet\")\n",
    "    \n",
    "    df = c.load_data(\"2-eval\", index)\n",
    "    \n",
    "    df_flows = pd.DataFrame()\n",
    "    \n",
    "    logger.debug(\"features_ds(): create bidirectional flowstats and calc directed ps and piats\")\n",
    "    \n",
    "    # We go through all possible packet counts. In this way we have fix number of columns and easily can calculate statistics on them.\n",
    "    for packet_count in range(2,30+1):\n",
    "        logger.debug(\"features_ds():    packet_count: {}\".format(packet_count))\n",
    "        df_tmp = df[df[\"PPI_LEN\"] == packet_count][c.pstats].copy()\n",
    "        columns_piat = [f\"PIAT_{i+1}\" for i in range(1,packet_count)]\n",
    "        columns_ps = [f\"PS_{i+1}\" for i in range(0,packet_count)]\n",
    "        \n",
    "        # SUBPSTATS\n",
    "        for i in range(1,30+1):\n",
    "            df_tmp[f\"SRC_PS_{i}\"] = 0\n",
    "            df_tmp[f\"SRC_PIAT_{i}\"] = 0\n",
    "            df_tmp[f\"DST_PS_{i}\"] = 0\n",
    "            df_tmp[f\"DST_PIAT_{i}\"] = 0\n",
    "        \n",
    "        # BIDIRECTIONAL SUBFLOWSTATS\n",
    "\n",
    "        df_tmp[\"bidirectional_duration_ms\"] = df_tmp[columns_piat].sum(axis=1)\n",
    "        df_tmp[\"bidirectional_packets\"] = packet_count\n",
    "        df_tmp[\"bidirectional_bytes\"] = df_tmp[columns_ps].sum(axis=1)\n",
    "\n",
    "        df_tmp[\"bidirectional_min_piat_ms\"] = df_tmp[columns_piat].min(axis=1)\n",
    "        df_tmp[\"bidirectional_max_piat_ms\"] = df_tmp[columns_piat].max(axis=1)\n",
    "        df_tmp[\"bidirectional_mean_piat_ms\"] = df_tmp[columns_piat].mean(axis=1)\n",
    "        \n",
    "        if packet_count == 2:\n",
    "            df_tmp[\"bidirectional_stddev_piat_ms\"] = 0\n",
    "        else:\n",
    "            df_tmp[\"bidirectional_stddev_piat_ms\"] = df_tmp[columns_piat].std(axis=1)\n",
    "\n",
    "        df_tmp[\"bidirectional_min_ps\"] = df_tmp[columns_ps].min(axis=1)\n",
    "        df_tmp[\"bidirectional_max_ps\"] = df_tmp[columns_ps].max(axis=1)\n",
    "        df_tmp[\"bidirectional_mean_ps\"] = df_tmp[columns_ps].mean(axis=1)\n",
    "        df_tmp[\"bidirectional_stddev_ps\"] = df_tmp[columns_ps].std(axis=1)\n",
    "        \n",
    "        # SUBPSTATS\n",
    "        \n",
    "        df_tmp[\"src_ps\"] = None\n",
    "        df_tmp[\"dst_ps\"] = None\n",
    "        df_tmp[\"src_piat\"] = None\n",
    "        df_tmp[\"dst_piat\"] = None\n",
    "        \n",
    "        for index, row in df_tmp.iterrows():\n",
    "            src_ps, src_piat = [], []\n",
    "            dst_ps, dst_piat = [], []\n",
    "            \n",
    "            # PS\n",
    "            for i in range(0, packet_count):\n",
    "                if row[f\"DIR_{i+1}\"] == 1:\n",
    "                    src_ps.append(row[f\"PS_{i+1}\"])\n",
    "                    df_tmp.at[index, f\"SRC_PS_{len(src_ps)}\"] = row[f\"PS_{i+1}\"]\n",
    "                else:\n",
    "                    dst_ps.append(row[f\"PS_{i+1}\"])\n",
    "                    df_tmp.at[index, f\"DST_PS_{len(dst_ps)}\"] = row[f\"PS_{i+1}\"]\n",
    "            df_tmp.at[index, \"src_ps\"] = src_ps\n",
    "            df_tmp.at[index, \"dst_ps\"] = dst_ps\n",
    "            \n",
    "            # PIAT\n",
    "            piat_src_wait_sum = 0\n",
    "            piat_dst_wait_sum = 0\n",
    "            \n",
    "            # FIRST PIAT ALWAYS SRC AND 0\n",
    "            df_tmp.at[index, f\"SRC_PIAT_1\"] = 0\n",
    "            for i in range(1, packet_count):\n",
    "                \n",
    "                piat = row[f\"PIAT_{i+1}\"]\n",
    "                # The DIR is SRC\n",
    "                if row[f\"DIR_{i+1}\"] == 1:\n",
    "                    piat_dst_wait_sum += piat\n",
    "                    src_piat.append(piat_src_wait_sum + piat)\n",
    "                    df_tmp.at[index, f\"SRC_PIAT_{len(src_piat)+1}\"] = piat_src_wait_sum + piat\n",
    "                    piat_src_wait_sum = 0\n",
    "                # The DIR is DST\n",
    "                else:\n",
    "                    piat_src_wait_sum += piat\n",
    "                    dst_piat.append(piat_dst_wait_sum + piat)\n",
    "                    df_tmp.at[index, f\"DST_PIAT_{len(dst_piat)}\"] = piat_dst_wait_sum + piat\n",
    "                    piat_dst_wait_sum = 0\n",
    "            \n",
    "            \n",
    "            df_tmp.at[index, \"src_piat\"] = src_piat\n",
    "            df_tmp.at[index, \"dst_piat\"] = dst_piat\n",
    "        \n",
    "        # SRC SUBFLOWSTATS\n",
    "        \n",
    "        df_tmp[\"src2dst_packets\"] = df_tmp[\"src_ps\"].apply(lambda x: len(x))\n",
    "        \n",
    "        \n",
    "        df_tmp[\"src2dst_bytes\"] = df_tmp[\"src_ps\"].apply(lambda x: np.sum(x) if len(x) > 0 else 0)\n",
    "        \n",
    "        df_tmp[\"src2dst_min_ps\"] = df_tmp[\"src_ps\"].apply(lambda x: np.min(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"src2dst_max_ps\"] = df_tmp[\"src_ps\"].apply(lambda x: np.max(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"src2dst_mean_ps\"] = df_tmp[\"src_ps\"].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"src2dst_stddev_ps\"] = df_tmp[\"src_ps\"].apply(lambda x: np.std(x) if len(x) > 0 else 0)\n",
    "        \n",
    "        \n",
    "        df_tmp[\"src2dst_duration_ms\"] = df_tmp[\"src_piat\"].apply(lambda x: np.sum(x) if len(x) > 0 else 0)\n",
    "\n",
    "        df_tmp[\"src2dst_min_piat_ms\"] = df_tmp[\"src_piat\"].apply(lambda x: np.min(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"src2dst_max_piat_ms\"] = df_tmp[\"src_piat\"].apply(lambda x: np.max(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"src2dst_mean_piat_ms\"] = df_tmp[\"src_piat\"].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"src2dst_stddev_piat_ms\"] = df_tmp[\"src_piat\"].apply(lambda x: np.std(x) if len(x) > 0 else 0)\n",
    "        \n",
    "        \n",
    "        # DST SUBFLOWSTATS\n",
    "        \n",
    "        df_tmp[\"dst2src_packets\"] = df_tmp[\"dst_ps\"].apply(lambda x: len(x))\n",
    "        \n",
    "        \n",
    "        df_tmp[\"dst2src_bytes\"] = df_tmp[\"dst_ps\"].apply(lambda x: np.sum(x) if len(x) > 0 else 0)\n",
    "        \n",
    "        df_tmp[\"dst2src_min_ps\"] = df_tmp[\"dst_ps\"].apply(lambda x: np.min(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"dst2src_max_ps\"] = df_tmp[\"dst_ps\"].apply(lambda x: np.max(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"dst2src_mean_ps\"] = df_tmp[\"dst_ps\"].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"dst2src_stddev_ps\"] = df_tmp[\"dst_ps\"].apply(lambda x: np.std(x) if len(x) > 0 else 0)\n",
    "        \n",
    "        df_tmp[\"dst2src_duration_ms\"] = df_tmp[\"dst_piat\"].apply(lambda x: np.sum(x) if len(x) > 0 else 0)\n",
    "        \n",
    "        df_tmp[\"dst2src_min_piat_ms\"] = df_tmp[\"dst_piat\"].apply(lambda x: np.min(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"dst2src_max_piat_ms\"] = df_tmp[\"dst_piat\"].apply(lambda x: np.max(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"dst2src_mean_piat_ms\"] = df_tmp[\"dst_piat\"].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "        df_tmp[\"dst2src_stddev_piat_ms\"] = df_tmp[\"dst_piat\"].apply(lambda x: np.std(x) if len(x) > 0 else 0)\n",
    "        \n",
    "        del df_tmp[\"src_ps\"]\n",
    "        del df_tmp[\"dst_ps\"]\n",
    "        del df_tmp[\"src_piat\"]\n",
    "        del df_tmp[\"dst_piat\"]\n",
    "        \n",
    "        df_flows = pd.concat([df_flows, df_tmp])\n",
    "        \n",
    "        del df_tmp\n",
    "    \n",
    "    logger.debug(\"flow_ds(): concat\")\n",
    "    df_flows = pd.concat([df, df_flows[c.pflowstats+c._pstats_subdirs]], axis=1)\n",
    "    \n",
    "    logger.debug(\"flow_ds(): to parquet\")\n",
    "    df_flows.to_parquet(PATH_DS_OUTPUT)\n",
    "    \n",
    "    logger.debug(\"flow_ds(): Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 15:54:08,248 [DEBUG] features_ds(): index: 1\n",
      "2025-06-04 15:54:09,045 [DEBUG] features_ds(): create bidirectional flowstats and calc directed ps and piats\n",
      "2025-06-04 15:54:09,046 [DEBUG] features_ds():    packet_count: 2\n",
      "2025-06-04 15:54:09,121 [DEBUG] features_ds():    packet_count: 3\n",
      "2025-06-04 15:54:09,151 [DEBUG] features_ds():    packet_count: 4\n",
      "2025-06-04 15:54:09,342 [DEBUG] features_ds():    packet_count: 5\n",
      "2025-06-04 15:54:09,372 [DEBUG] features_ds():    packet_count: 6\n",
      "2025-06-04 15:54:09,426 [DEBUG] features_ds():    packet_count: 7\n",
      "2025-06-04 15:54:10,374 [DEBUG] features_ds():    packet_count: 8\n",
      "2025-06-04 15:54:10,788 [DEBUG] features_ds():    packet_count: 9\n",
      "2025-06-04 15:54:12,990 [DEBUG] features_ds():    packet_count: 10\n",
      "2025-06-04 15:54:19,504 [DEBUG] features_ds():    packet_count: 11\n",
      "2025-06-04 15:54:24,000 [DEBUG] features_ds():    packet_count: 12\n",
      "2025-06-04 15:54:26,875 [DEBUG] features_ds():    packet_count: 13\n",
      "2025-06-04 15:54:32,219 [DEBUG] features_ds():    packet_count: 14\n",
      "2025-06-04 15:54:39,375 [DEBUG] features_ds():    packet_count: 15\n",
      "2025-06-04 15:54:48,271 [DEBUG] features_ds():    packet_count: 16\n",
      "2025-06-04 15:54:57,068 [DEBUG] features_ds():    packet_count: 17\n",
      "2025-06-04 15:55:10,799 [DEBUG] features_ds():    packet_count: 18\n",
      "2025-06-04 15:55:23,515 [DEBUG] features_ds():    packet_count: 19\n",
      "2025-06-04 15:55:34,863 [DEBUG] features_ds():    packet_count: 20\n",
      "2025-06-04 15:55:45,647 [DEBUG] features_ds():    packet_count: 21\n",
      "2025-06-04 15:55:54,167 [DEBUG] features_ds():    packet_count: 22\n",
      "2025-06-04 15:56:01,945 [DEBUG] features_ds():    packet_count: 23\n",
      "2025-06-04 15:56:09,369 [DEBUG] features_ds():    packet_count: 24\n",
      "2025-06-04 15:56:17,582 [DEBUG] features_ds():    packet_count: 25\n",
      "2025-06-04 15:56:28,303 [DEBUG] features_ds():    packet_count: 26\n",
      "2025-06-04 15:56:40,583 [DEBUG] features_ds():    packet_count: 27\n",
      "2025-06-04 15:56:51,746 [DEBUG] features_ds():    packet_count: 28\n",
      "2025-06-04 15:57:00,395 [DEBUG] features_ds():    packet_count: 29\n",
      "2025-06-04 15:57:07,610 [DEBUG] features_ds():    packet_count: 30\n",
      "2025-06-04 16:00:52,059 [DEBUG] flow_ds(): concat\n",
      "2025-06-04 16:00:53,823 [DEBUG] flow_ds(): to parquet\n",
      "2025-06-04 16:00:58,261 [DEBUG] flow_ds(): Done!\n",
      "2025-06-04 16:00:58,783 [DEBUG] features_ds(): index: 2\n",
      "2025-06-04 16:00:59,445 [DEBUG] features_ds(): create bidirectional flowstats and calc directed ps and piats\n",
      "2025-06-04 16:00:59,446 [DEBUG] features_ds():    packet_count: 2\n",
      "2025-06-04 16:00:59,517 [DEBUG] features_ds():    packet_count: 3\n",
      "2025-06-04 16:00:59,563 [DEBUG] features_ds():    packet_count: 4\n",
      "2025-06-04 16:00:59,874 [DEBUG] features_ds():    packet_count: 5\n",
      "2025-06-04 16:01:00,023 [DEBUG] features_ds():    packet_count: 6\n",
      "2025-06-04 16:01:00,131 [DEBUG] features_ds():    packet_count: 7\n",
      "2025-06-04 16:01:01,131 [DEBUG] features_ds():    packet_count: 8\n",
      "2025-06-04 16:01:01,715 [DEBUG] features_ds():    packet_count: 9\n",
      "2025-06-04 16:01:03,376 [DEBUG] features_ds():    packet_count: 10\n",
      "2025-06-04 16:01:09,695 [DEBUG] features_ds():    packet_count: 11\n",
      "2025-06-04 16:01:15,922 [DEBUG] features_ds():    packet_count: 12\n",
      "2025-06-04 16:01:19,509 [DEBUG] features_ds():    packet_count: 13\n",
      "2025-06-04 16:01:25,507 [DEBUG] features_ds():    packet_count: 14\n"
     ]
    }
   ],
   "source": [
    "for index in range(1, 28+1):\n",
    "    features_ds(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Preprocess and create DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_ds(start_day=15, end_day=28):\n",
    "    logger.info(\"Creating dataset\")\n",
    "    os.makedirs(path(c.path_dataset, \"3-features\"), exist_ok=True)\n",
    "    os.makedirs(path(c.path_results, \"class_labeling\"), exist_ok=True)\n",
    "    \n",
    "    day_interval = range(start_day, end_day+1)\n",
    "    starting_timestamp = pd.Timestamp('2022-11-14 00:00:00.000000')\n",
    "    \n",
    "    features = c.flowstats + c.pstats + c.pflowstats + c._pstats_subdirs\n",
    "    columns = [\"TIME_LAST\", c.app, c.category, c.org] + features\n",
    "    \n",
    "    logger.info(\" Loading data...\")\n",
    "    # Load all data\n",
    "    df = pd.concat(\n",
    "        # Load each day and concat then save\n",
    "        [pd.read_parquet(path(c.path_dataset, \"3-features\", f\"day-{i}.parquet\"), columns=columns) for i in day_interval ], ignore_index=True, copy=False\n",
    "    )\n",
    "    \n",
    "    # Datetime things\n",
    "    logger.info(\" Dropping rows with invalid TIME_LAST\")\n",
    "    len_df_1 = len(df) \n",
    "    df['TIME_LAST'] = pd.to_datetime(df['TIME_LAST'], format=\"%Y-%m-%dT%H:%M:%S.%f\", errors='coerce')\n",
    "    df = df.dropna(subset=['TIME_LAST'])\n",
    "    len_df_2 = len(df) \n",
    "    logger.info(f\" Dropped {len_df_1 - len_df_2} rows\")\n",
    "    \n",
    "    logger.info(\" Dropping flows which are exported before 2022-11-14 00:00:00.000000\")\n",
    "    df = df[df[\"TIME_LAST\"] >= starting_timestamp]\n",
    "    len_df_3 = len(df) \n",
    "    logger.info(f\" Dropped {len_df_2 - len_df_3} rows\")\n",
    "    \n",
    "    logger.info(\" Sorting by TIME_LAST\")\n",
    "    df = df.sort_values(by=\"TIME_LAST\")\n",
    "    \n",
    "    # Encode\n",
    "    logger.info(\" Label encoding\")\n",
    "    encoder = LabelEncoder()\n",
    "    encoder = encoder.fit(df[\"APP\"])\n",
    "    classes = encoder.classes_\n",
    "    \n",
    "    class_to_integer = {cls: idx for idx, cls in enumerate(classes)}\n",
    "    integer_to_class = {idx: cls for idx, cls in enumerate(classes)}\n",
    "    class_to_category = dict(zip(df[\"APP\"], df[\"CATEGORY\"]))\n",
    "    integer_to_category = {idx: class_to_category[cls] for idx, cls in enumerate(classes)}\n",
    "\n",
    "    with open(path(c.path_results, \"class_labeling\", 'cls_to_int.json'), 'w') as class_to_int_file:\n",
    "        json.dump(class_to_integer, class_to_int_file, indent=4)\n",
    "\n",
    "    with open(path(c.path_results, \"class_labeling\", 'int_to_cls.json'), 'w') as int_to_class_file:\n",
    "        json.dump(integer_to_class, int_to_class_file, indent=4)\n",
    "        \n",
    "    with open(path(c.path_results, \"class_labeling\", 'cls_to_cat.json'), 'w') as class_to_category_file:\n",
    "        json.dump(class_to_category, class_to_category_file, indent=4)\n",
    "        \n",
    "    with open(path(c.path_results, \"class_labeling\", 'int_to_cat.json'), 'w') as int_to_category_file:\n",
    "        json.dump(integer_to_category, int_to_category_file, indent=4)\n",
    "    \n",
    "    df[\"APP\"] = encoder.transform(df[\"APP\"])\n",
    "    \n",
    "    # Scale\n",
    "    logger.info(\" Standard scaling dataset\")\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "    \n",
    "    logger.info(\" Saving dataset...\")\n",
    "    df.to_parquet(path(c.path_dataset, \"4-dataset\", f\"dataset.parquet\"))\n",
    "    \n",
    "    logger.info(\" Saving dataset by ORG_ID ...\")\n",
    "    org = 1\n",
    "    with tqdm(df[c.org].unique(), f\"Organizations {org}/{len(df[c.org].unique())}\") as orgs:\n",
    "        for org_id in orgs:\n",
    "            df[df[c.org] == org_id].to_parquet(path(c.path_dataset, \"4-dataset\", f\"org-{org_id}.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5A - Create Federated simulation DS-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_world_A(time_window):\n",
    "    scenario = \"A\"\n",
    "    for org_id in range(1, 14+1):\n",
    "        os.makedirs(path(c.path_dataset, \"5-federated\", f\"realworld-{scenario}\", f\"client-{org_id}\"), exist_ok=True)\n",
    "        df_org = pd.read_parquet(path(c.path_dataset, \"4-dataset\", f\"org-{org_id}.parquet\"), columns=c.columns+[\"TIME_LAST\"])\n",
    "\n",
    "        # Sort by TIME_LAST timestamp\n",
    "        df_org.loc[:, 'TIME_LAST'] = pd.to_datetime(df_org['TIME_LAST'], format=\"%Y-%m-%dT%H:%M:%S.%f\", errors='coerce')\n",
    "        df_org = df_org.dropna(subset=['TIME_LAST'])\n",
    "\n",
    "        df_org = df_org.set_index('TIME_LAST')\n",
    "        df_org = df_org.sort_values(by='TIME_LAST')\n",
    "        \n",
    "        chunk = 1\n",
    "        # Sample dataset by a time window\n",
    "        for timestamp, group in df_org.resample(time_window):\n",
    "            # 70-20-10 train-test-validation split\n",
    "            train, test = train_test_split(group, test_size=0.2, random_state=c.random_state)\n",
    "            train, validation = train_test_split(train, test_size=0.125, random_state=c.random_state)\n",
    "        \n",
    "            train.to_parquet(path(c.path_dataset, \"5-federated\", f\"realworld-{scenario}\", f\"client-{org_id}\", f\"train-chunk-{chunk}.parquet\"))\n",
    "            validation.to_parquet(path(c.path_dataset, \"5-federated\", f\"realworld-{scenario}\", f\"client-{org_id}\", f\"validation-chunk-{chunk}.parquet\"))\n",
    "            test.to_parquet(path(c.path_dataset, \"5-federated\", f\"realworld-{scenario}\", f\"client-{org_id}\", f\"test-chunk-{chunk}.parquet\"))\n",
    "            chunk += 1\n",
    "            \n",
    "real_world_A(\"3h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize train chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "scenario = \"realworld-A\"\n",
    "\n",
    "data = np.zeros((14, 112))\n",
    "for run in range(1, 112+1):\n",
    "    for org_id in range(1,14+1):\n",
    "        try:\n",
    "            df = pd.read_parquet(path(c.path_dataset, \"5-federated\", scenario, f\"client-{org_id}\", f\"train-chunk-{run}.parquet\"), columns=c.appl)\n",
    "            data[org_id-1, run-1] = len(df)\n",
    "        except:\n",
    "            data[org_id-1, run-1] = 0\n",
    "\n",
    "data = data.astype(int)\n",
    "ax = sns.heatmap(\n",
    "    data,\n",
    "    cmap=\"YlGnBu\",\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cbar=True\n",
    ")\n",
    "\n",
    "for text in ax.texts:\n",
    "    text.set_rotation(90)\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5B - Create Federated simulation DS-B with buffer mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Buffer\n",
    "def real_world_B(time_window):\n",
    "    scenario = \"B\"\n",
    "    buffered_rows_train_max = c.batch_size_client * 100\n",
    "    buffered_rows_test_max = int(buffered_rows_train_max / 0.7 * 0.2)\n",
    "    buffered_rows_validation_max = int(buffered_rows_train_max / 0.7 * 0.1)\n",
    "    for org_id in range(1, 14+1):\n",
    "        os.makedirs(path(c.path_dataset, \"5-federated\", f\"realworld-{scenario}\", f\"client-{org_id}\"), exist_ok=True)\n",
    "        df_org = pd.read_parquet(path(c.path_dataset, \"4-dataset\", f\"org-{org_id}.parquet\"), columns=c.columns+[\"TIME_LAST\"])\n",
    "    \n",
    "        df_org.loc[:, 'TIME_LAST'] = pd.to_datetime(df_org['TIME_LAST'], format=\"%Y-%m-%dT%H:%M:%S.%f\", errors='coerce')\n",
    "        df_org = df_org.dropna(subset=['TIME_LAST'])\n",
    "\n",
    "        df_org = df_org.set_index('TIME_LAST')\n",
    "        df_org = df_org.sort_values(by='TIME_LAST')\n",
    "        \n",
    "        chunk = 1\n",
    "        buffered_train = pd.DataFrame()\n",
    "        buffered_test = pd.DataFrame()\n",
    "        buffered_validation = pd.DataFrame()\n",
    "        for timestamp, group in df_org.resample(time_window):\n",
    "            \n",
    "            train, test = train_test_split(group, test_size=0.2, random_state=c.random_state)    \n",
    "            train, validation = train_test_split(train, test_size=0.125, random_state=c.random_state)\n",
    "            \n",
    "            buffered_train = pd.concat([buffered_train, train], ignore_index=True, copy=False)\n",
    "            buffered_test = pd.concat([buffered_test, test], ignore_index=True, copy=False)\n",
    "            buffered_validation = pd.concat([buffered_validation, validation], ignore_index=True, copy=False)\n",
    "            \n",
    "            total_train_rows = len(buffered_train)\n",
    "            total_test_rows = len(buffered_test)\n",
    "            total_validation_rows = len(buffered_validation)\n",
    "            \n",
    "            # There is not enough train rows buffered, export as test\n",
    "            if total_train_rows < buffered_rows_train_max:\n",
    "                buffered_rows = pd.concat([buffered_train, buffered_test, buffered_validation], ignore_index=True, copy=False)\n",
    "                buffered_rows.to_parquet(path(c.path_dataset, \"5-federated\", f\"realworld-{scenario}\", f\"client-{org_id}\", f\"test-chunk-{chunk}.parquet\"))\n",
    "            # There is much more rows than needed.\n",
    "            else:\n",
    "                rows_to_remove_from_train = total_train_rows - buffered_rows_train_max\n",
    "                rows_to_remove_from_test = total_test_rows - buffered_rows_test_max\n",
    "                rows_to_remove_from_validation = total_validation_rows - buffered_rows_validation_max\n",
    "                \n",
    "                buffered_train = buffered_train.iloc[rows_to_remove_from_train:]\n",
    "                buffered_test = buffered_test.iloc[rows_to_remove_from_test:]\n",
    "                buffered_validation = buffered_validation.iloc[rows_to_remove_from_validation:]\n",
    "                \n",
    "                buffered_train.to_parquet(path(c.path_dataset, \"5-federated\", f\"realworld-{scenario}\", f\"client-{org_id}\", f\"train-chunk-{chunk}.parquet\"))\n",
    "                buffered_test.to_parquet(path(c.path_dataset, \"5-federated\", f\"realworld-{scenario}\", f\"client-{org_id}\", f\"test-chunk-{chunk}.parquet\"))\n",
    "                buffered_validation.to_parquet(path(c.path_dataset, \"5-federated\", f\"realworld-{scenario}\", f\"client-{org_id}\", f\"validation-chunk-{chunk}.parquet\"))\n",
    "                \n",
    "            chunk += 1\n",
    "            \n",
    "            \n",
    "real_world_B(\"3h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize train chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "scenario = \"realworld-A\"\n",
    "\n",
    "data = np.zeros((14, 112))\n",
    "for run in range(1, 112+1):\n",
    "    for org_id in range(1,14+1):\n",
    "        try:\n",
    "            df = pd.read_parquet(path(c.path_dataset, \"5-federated\", scenario, f\"client-{org_id}\", f\"train-chunk-{run}.parquet\"), columns=c.appl)\n",
    "            data[org_id-1, run-1] = len(df)\n",
    "        except:\n",
    "            data[org_id-1, run-1] = 0\n",
    "\n",
    "data = data.astype(int)\n",
    "ax = sns.heatmap(\n",
    "    data,\n",
    "    cmap=\"YlGnBu\",\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cbar=True\n",
    ")\n",
    "\n",
    "for text in ax.texts:\n",
    "    text.set_rotation(90)\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dipterv_python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
